<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="概述 优化算法对于深度学习非常重要。一方面，训练一个复杂的深度学习模型可能需要数小时、数天甚至数周的时间。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原理及其超参数的作用，将使我们能够有针对性地调优超参数，以提高深度学习模型的性能[1]。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习优化算法">
<meta property="og:url" content="http://example.com/2022/11/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="Scofield">
<meta property="og:description" content="概述 优化算法对于深度学习非常重要。一方面，训练一个复杂的深度学习模型可能需要数小时、数天甚至数周的时间。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原理及其超参数的作用，将使我们能够有针对性地调优超参数，以提高深度学习模型的性能[1]。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://miro.medium.com/max/1027/1*6MEi74EMyPERHlAX-x2Slw.png">
<meta property="og:image" content="https://wikimedia.org/api/rest_v1/media/math/render/svg/4eb9bb54b2820fb3583901ec05bc4b474b6d90bc">
<meta property="og:image" content="https://discoverml.github.io/simplified-deeplearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/img/algorithm8.4.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190814171722940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FwcGxleXVjaGk=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://discoverml.github.io/simplified-deeplearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/img/algorithm8.5.png">
<meta property="og:image" content="https://discoverml.github.io/simplified-deeplearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/img/algorithm8.7.png">
<meta property="og:image" content="https://ml-explained.com/articles/adamw-explained/adamw.png">
<meta property="og:image" content="https://ml-explained.com/_ipx/sizes_xs:320px%20md:768px%20lg:1024px,w_1536,f_png/articles/radam-explained/radam_update_rule.png">
<meta property="article:published_time" content="2022-11-16T07:18:11.000Z">
<meta property="article:modified_time" content="2022-11-20T03:17:09.804Z">
<meta property="article:author" content="Scofield">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://miro.medium.com/max/1027/1*6MEi74EMyPERHlAX-x2Slw.png">


<link rel="canonical" href="http://example.com/2022/11/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2022/11/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/","path":"2022/11/16/深度学习优化算法/","title":"深度学习优化算法"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习优化算法 | Scofield</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Scofield</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tag fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8D%E4%BD%BF%E7%94%A8%E7%89%9B%E9%A1%BF%E6%B3%95%E6%88%96%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95a"><span class="nav-number">1.1.</span> <span class="nav-text">为什么深度学习不使用牛顿法或拟牛顿法？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.</span> <span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#stochastic-gradient-descent"><span class="nav-number">2.1.</span> <span class="nav-text">Stochastic gradient descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mini-batch-gradient-descent"><span class="nav-number">2.2.</span> <span class="nav-text">mini-batch gradient descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%91%E6%88%98a"><span class="nav-number">2.3.</span> <span class="nav-text">挑战</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F"><span class="nav-number">2.4.</span> <span class="nav-text">动量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nesterov-accelerated-gradient-nag"><span class="nav-number">2.5.</span> <span class="nav-text">Nesterov accelerated gradient (NAG)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#adagrad2011"><span class="nav-number">3.</span> <span class="nav-text">AdaGrad2011</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#adadelta2012"><span class="nav-number">4.</span> <span class="nav-text">Adadelta2012</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rmsprop2012"><span class="nav-number">5.</span> <span class="nav-text">RMSProp2012</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#adam2014"><span class="nav-number">6.</span> <span class="nav-text">Adam2014</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#adamw2018"><span class="nav-number">7.</span> <span class="nav-text">AdamW2018</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#radam2019"><span class="nav-number">8.</span> <span class="nav-text">RAdam2019</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">9.</span> <span class="nav-text">参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Scofield"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Scofield</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/just-do-it-ai" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;just-do-it-ai" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xluo2583@gmail.com" title="E-Mail → mailto:xluo2583@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/images/wechat.jpg" title="WeChat → &#x2F;images&#x2F;wechat.jpg"><i class="fa fa-weixin fa-fw"></i>WeChat</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/11/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Scofield">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Scofield">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习优化算法 | Scofield">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习优化算法
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-11-16 15:18:11" itemprop="dateCreated datePublished" datetime="2022-11-16T15:18:11+08:00">2022-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-11-20 11:17:09" itemprop="dateModified" datetime="2022-11-20T11:17:09+08:00">2022-11-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="概述">概述</h1>
<p>优化算法对于深度学习非常重要。一方面，训练一个复杂的深度学习模型可能需要数小时、数天甚至数周的时间。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原理及其超参数的作用，将使我们能够有针对性地调优超参数，以提高深度学习模型的性能[1]。<span id="more"></span><br></p>
<p>优化算法的分类有很多种方法，其中一种方法可以根据目标函数是否可微[2]。<br></p>
<ol type="1">
<li><p>目标函数可微时，优化方法又可分为一阶梯度优化算法和二阶梯度优化算法等。一节梯度优化算法利用一阶导数(梯度)来选择在搜索空间中移动的方向。常见的一阶梯度优化算法包括：</p>
<ul>
<li>Gradient Descent</li>
<li>Momentum</li>
<li>Adagrad</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
<p>二阶梯度算法使用二阶导数(Hessian)来选择在搜索空间中的移动方向。常见的二阶梯度优化方法包括：</p>
<ul>
<li>Newton’s Method牛顿方法</li>
<li>Secant Method割线法</li>
<li>Quasi-Newton Method拟牛顿法</li>
</ul></li>
<li><p>目标函数不可微分时，常见的优化算法可以分为：</p></li>
</ol>
<ul>
<li><p>Direct Algorithms</p>
<blockquote>
<p>直接优化算法举例：<br> 1.Cyclic Coordinate Search<br> 2.Powell’s Method<br> 3.Hooke-Jeeves Method<br> 4.Nelder-Mead Simplex Search<br></p>
</blockquote></li>
<li><p>Stochastic Optimization Algorithms</p>
<blockquote>
<p>随机优化算法举例：<br> 1.Simulated Annealing<br> 2.Evolution Strategy<br> 3.Cross-Entropy Method<br></p>
</blockquote></li>
<li><p>Population optimization algorithms</p>
<blockquote>
<p>种群优化算法举例：<br> 1.Genetic Algorithm遗传算法<br> 2.Differential Evolution<br> 3.Particle Swarm Optimization<br></p>
</blockquote></li>
</ul>
<h2 id="为什么深度学习不使用牛顿法或拟牛顿法a"><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011094454/article/details/79256147">为什么深度学习不使用牛顿法或拟牛顿法？</a></h2>
<p>原因一：牛顿法需要用到二阶梯度和Hessian矩阵，这两个都难以求解。因为很难写出深度神经网络拟合函数的表达式，遑论直接得到其梯度表达式，更不要说得到基于梯度的Hessian矩阵了。<br> 原因二：即使可以得到梯度和Hessian矩阵，当输入向量的维度N较大时，Hessian矩阵的大小是N×N，所需要的内存非常大。<br> 原因三：在高维非凸优化问题中，鞍点相对于局部最小值的数量非常多，而且鞍点处的损失值相对于局部最小值处也比较大。而二阶优化算法是寻找梯度为0的点，所以很容易陷入鞍点。<br></p>
<h1 id="梯度下降">梯度下降</h1>
<p>梯度下降有三种变体，之间的区别是我们使用多少数据来计算目标函数的梯度。根据数据量的不同，我们在参数更新的准确性和执行更新所需的时间之间进行权衡。 ## batch gradient descent batch gradient descent在整个数据集上计算损失函数的梯度并更新参数。 <span class="math display">\[\theta=\theta-\eta \cdot \nabla_{\theta} J(\theta)\]</span> 由于我们需要计算整个数据集的梯度来执行一次更新，批处理梯度下降可能非常慢，并且对内存的需求的非常大。由于内存的限制，有些数据集无法采用该方法进行梯度更新。<br> 批量梯度下降保证了凸误差曲面收敛到全局最小值，非凸误差曲面收敛到局部最小值。</p>
<h2 id="stochastic-gradient-descent">Stochastic gradient descent</h2>
<p>SGD对每一个随机样本更新一次参数。 <span class="math display">\[
\theta=\theta-\eta \cdot \nabla_{\theta} J\left(\theta ; x^{(i)} ; y^{(i)}\right)
\]</span> <strong>优点：</strong> batch gradient descent在大型数据集上会有许多冗余计算，因为它在每次参数更新之前为类似的示例重新计算梯度。SGD通过每次计算一个样本数据来消除这种冗余。SGD通常更快，也可以用于在线学习。</p>
<p><strong>缺点：</strong> 因为只计算一个样本，所以容易产生梯度更新过程比较震荡。另一方面却可以帮助模型跳出到一个可能的局部最优点。</p>
<p>缓慢的减少学习率，SGD可以达到和batch gradient descent一样的收敛性能。</p>
<h2 id="mini-batch-gradient-descent">mini-batch gradient descent</h2>
<p>mini-batch gradient descent在小批量的数据集上计算梯度并更新参数。 <span class="math display">\[
\theta=\theta-\eta \cdot \nabla_{\theta} J\left(\theta ; x^{(i:i+n)} ; y^{(i:i+n)}\right)
\]</span> mini-batch gradient descent减少了参数更新的方差，收敛更稳定;同时，可以利用深度学习库中常见的高度优化矩阵优化，使计算小批量梯度非常高效。 深度学习训练中，SGD通常和mini-batch捆绑。</p>
<h2 id="挑战a"><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011094454/article/details/79256147">挑战</a></h2>
<ul>
<li>选择一个合适的学习率是很困难的。学习率过低会导致极其缓慢的收敛，而学习率过高则会阻碍收敛，导致损失函数在最小值附近波动甚至发散。</li>
<li>学习率计划(Learning rate schedules)试图通过退火(annealing)等方法调整训练期间的学习率，即根据预定义的方法或在每个epoch的目标变化低于阈值时降低学习率。然而，这些schedules和阈值必须预先定义，因此无法适应数据集的特征。</li>
<li>如果我们的数据是稀疏的，我们的特征有非常不同的频率，我们可能不想以相同的学习率更新所有的参数，而是对于频率高的参数用小的学习率更新，频率低的特征用大的学习率更新。</li>
<li>局部最小值和鞍点的存在使得SGD很难逃脱。尤其是鞍点，其在所有维度上的梯度都为0。</li>
</ul>
<h2 id="动量">动量</h2>
<p><span class="math display">\[
v_{t} =\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta \right) 
\]</span> <span class="math display">\[
\theta =\theta-v_{t}
\]</span> <span class="math inline">\(\gamma\)</span>是动量超参数，通常设置为0.9。<span class="math inline">\(\eta\)</span>是学习率，<span class="math inline">\(\theta\)</span>是学习的参数。$v_{t} =v_{t-1}+_{}J() <span class="math inline">\(类似于指数移动平均，如果\)</span>$等于<span class="math inline">\(1-\gamma\)</span>就是指数平均。</p>
<p>动量对过去时间的梯度更新有加权。梯度指向相同方向，动量项增加; 梯度方向不同，动量项的更新减少。因此可以加速模型的收敛，减少震荡。</p>
<p>在pytorch中momentum的部署不太一样： <span class="math display">\[v_{t}=\gamma v_{t-1}+g_{t}\]</span> <span class="math display">\[\theta=\theta-\eta  v_{t}\]</span></p>
<h2 id="nesterov-accelerated-gradient-nag">Nesterov accelerated gradient (NAG)</h2>
<p><span class="math display">\[
\begin{aligned}
v_{t} =\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right) \end{aligned}
\]</span></p>
<p><span class="math display">\[
\theta =\theta-v_{t}
\]</span></p>
<p>计算<span class="math inline">\(\theta-\gamma v_{t-1}\)</span>可以告诉我们下一次参数更新的大致位置，这相当于提供给了模型参数更新的预见性，防止模型在动量的加持下更新过快。</p>
<p><img src="https://miro.medium.com/max/1027/1*6MEi74EMyPERHlAX-x2Slw.png" /></p>
<h1 id="adagrad2011">AdaGrad2011</h1>
<p>Ada表示adaptive自适应。 我们对所有参数 <span class="math inline">\(\theta\)</span> 进行一次更新，因为每个参数 <span class="math inline">\(\theta_i\)</span> 使用相同的学习率 <span class="math inline">\(\eta\)</span>。由于Adagrad在每个时间步 <span class="math inline">\(t\)</span> 上对每个参数 <span class="math inline">\(\theta_i\)</span> 使用不同的学习率，我们首先显示Adagrad的每个参数更新，然后对其进行向量化。为简单起见，我们设<span class="math inline">\(g_{t,i}\)</span>为目标函数w.r.t在时间步 <span class="math inline">\(t\)</span> 时对参数 <span class="math inline">\(\theta_i\)</span> 的梯度: <span class="math display">\[
g_{t, i} =\nabla_{\theta_{t}} J\left(\theta_{t, i}\right) 
\]</span> 每个参数 <span class="math inline">\(\theta_i\)</span> 在每个时间步骤 <span class="math inline">\(t\)</span> 处的SGD更新为: <span class="math display">\[
\quad \theta_{t+1, i}=\theta_{t, i}-\eta \cdot g_{t, i} \quad 
\]</span> Adagrad在其更新规则中，根据为 <span class="math inline">\(\theta_i\)</span> 计算的过去梯度，修改了每个时间步 <span class="math inline">\(t\)</span> 下每个参数 <span class="math inline">\(\theta_i\)</span> 的一般学习率 <span class="math inline">\(\eta\)</span> <span class="math display">\[
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i} 
\]</span></p>
<p><span class="math inline">\(G_t\)</span> 是一个对角矩阵，其中每个对角元素 <span class="math inline">\(i,i\)</span> 是到时间步长为t的梯度 <span class="math inline">\(θ_i\)</span> 的平方和，<span class="math inline">\(\epsilon\)</span> 是一个平滑项，避免除零(通常是1e−8的量级)。</p>
<p>矩阵优化计算： <span class="math display">\[
\theta_{t+1} =\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}} \odot g_{t} .
\]</span></p>
<p><span class="math inline">\(\odot\)</span> 是Hadamard product，阿达玛乘积，其输入为两个相同形状的矩阵，输出是具有同样形状的、各个位置的元素等于两个输入矩阵相同位置元素的乘积的矩阵。</p>
<blockquote>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4eb9bb54b2820fb3583901ec05bc4b474b6d90bc" /></p>
</blockquote>
<p><img src="https://discoverml.github.io/simplified-deeplearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/img/algorithm8.4.png" /></p>
<p><strong>优点：</strong> 不再需要手动调整学习率。许多部署使用默认学习率0.01就行。</p>
<p><strong>缺点：</strong> Adagrad在分母中积累梯度的平方:因为每个添加的项都是正的，累积的总和在训练过程中不断增长。这反过来又会导致学习率缩小，最终变得无穷小，导致模型不再能够获得额外的知识。</p>
<h1 id="adadelta2012">Adadelta2012</h1>
<p>Adadelta解决了AdaGrad学习率急剧缩小的缺点。Adadelta并没有累积所有过去的平方梯度，而是将过去累积梯度的窗口限制为某个固定大小的 <span class="math inline">\(w\)</span>。<br> 相对于低效地存储 <span class="math inline">\(w\)</span> 个以前的平方梯度，梯度的和被递归定义为所有过去的平方梯度的加权平均值。指数移动平均 <span class="math inline">\(E\left[g^2\right]_t\)</span> 取决于<span class="math inline">\(t-1\)</span>时刻的平均值和当前的梯度。 <span class="math display">\[
E\left[g^2\right]_t=\gamma E\left[g^2\right]_{t-1}+(1-\gamma) g_t^2
\]</span> <span class="math inline">\(\gamma\)</span> 通常设置为0.9。SGD可以重新书写为： <span class="math display">\[
\begin{aligned}
\Delta \theta_t &amp;=-\eta \cdot g_{t, i} \\
\theta_{t+1} &amp;=\theta_t+\Delta \theta_t
\end{aligned}
\]</span></p>
<p>Adagrad中参数更新公式为: <span class="math display">\[
\Delta \theta_t=-\frac{\eta}{\sqrt{G_t+\epsilon}} \odot g_t
\]</span> Adadelta只是将 <span class="math inline">\(G_t\)</span> 替换成了 <span class="math inline">\(E\left[g^2\right]_t\)</span> : <span class="math display">\[
\Delta \theta_t=-\frac{\eta}{\sqrt{E\left[g^2\right]_t+\epsilon}} g_t
\]</span> root mean squared (RMS)表示对梯度求均方根误差，则可以简写为: <span class="math display">\[
\Delta \theta_t=-\frac{\eta}{R M S[g]_t} g_t
\]</span></p>
<p><img src="https://img-blog.csdnimg.cn/20190814171722940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FwcGxleXVjaGk=,size_16,color_FFFFFF,t_70" /></p>
<h1 id="rmsprop2012">RMSProp2012</h1>
<p>RMSProp也是一种自适应性学习率方法。其目的也是解决AdaGrad学习率急剧缩小的缺点。其公式和Adadelta一致。 <span class="math display">\[
\begin{aligned}
E\left[g^2\right]_t &amp;=0.9 E\left[g^2\right]_{t-1}+0.1 g_t^2 \\
\theta_{t+1} &amp;=\theta_t-\frac{\eta}{\sqrt{E\left[g^2\right]_t+\epsilon}} g_t
\end{aligned}
\]</span> <span class="math inline">\(\gamma\)</span> 设置成 <span class="math inline">\(0.9\)</span>, 学习率 <span class="math inline">\(\eta\)</span> 建议设置成<span class="math inline">\(0.001\)</span>。</p>
<p><img src="https://discoverml.github.io/simplified-deeplearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/img/algorithm8.5.png" /></p>
<h1 id="adam2014">Adam2014</h1>
<p>Adaptive Moment Estimation自适应矩估计是另一种计算自适应学习率的方法。<br> 除了像Adadelta和RMSprop那样存储过去平方梯度 <span class="math inline">\(v_t\)</span> 的指数衰减平均值之外，Adam还保存了过去梯度 <span class="math inline">\(m_t\)</span> 的指数衰减平均值，该值类似于动量: <span class="math display">\[
\begin{aligned}
m_t &amp;=\beta_1 m_{t-1}+\left(1-\beta_1\right) g_t \\
v_t &amp;=\beta_2 v_{t-1}+\left(1-\beta_2\right) g_t^2
\end{aligned}
\]</span> <span class="math inline">\(m_t\)</span> 和 <span class="math inline">\(v_t\)</span> 分别是梯度的一阶矩(均值)估计和二阶矩(方差(the uncentered variance))估计。 当 <span class="math inline">\(m_t\)</span> 和 <span class="math inline">\(v_t\)</span>初始化为 <span class="math inline">\(0\)</span> 并且 <span class="math inline">\(\beta_1\)</span> 和 <span class="math inline">\(\beta_2\)</span> 接近 <span class="math inline">\(1\)</span> 时，估计偏差比较大。为了修正偏差： <span class="math display">\[
\begin{aligned}
\hat{m}_t &amp;=\frac{m_t}{1-\beta_1^t} \\
\hat{v}_t &amp;=\frac{v_t}{1-\beta_2^t}
\end{aligned}
\]</span></p>
<p>然后，像RMSProp和Adagrad一样更新参数。 <span class="math display">\[
\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat{v}_t}+\epsilon} \hat{m}_t
\]</span></p>
<p><img src="https://discoverml.github.io/simplified-deeplearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/img/algorithm8.7.png" /></p>
<h1 id="adamw2018">AdamW2018</h1>
<p><span class="math inline">\(l_2\)</span>正则化和权重衰减在SGD中是一样的，但是在Adam中却不同。之前人们使用 <span class="math inline">\(l_2\)</span> 正则化替代权重衰减部署在Adam算法中，导致许多图像数据集中Adam相比于带有动量的SGD的结果更差。<br> 运行时间/要执行的批处理次数越大，最佳权重衰减越小。<br> Adam是一种自适应梯度算法，并因此适应每个参数的学习率，但这并不排除使用全局学习率乘法器(如余弦退火)大幅提高其性能的可能性。</p>
<blockquote>
<p>权重衰减的公式:<br> <span class="math inline">\(\theta_{t+1} = (1-\lambda)\theta_{t}-\alpha g_t\)</span> <br> <span class="math inline">\(\lambda\)</span> 是每步的权重衰减率。<span class="math inline">\(\alpha\)</span> 是学习率。<br> SGD中<span class="math inline">\(l_2\)</span>正则化： <span class="math inline">\(f(\theta) + \frac{\lambda^{&#39;}}{2}||\theta||_2^2\)</span>，如果<span class="math inline">\({\lambda^{&#39;}}=\frac{\lambda}{\alpha}\)</span>，则<span class="math inline">\(l_2\)</span>正则化与权重衰减一样。</p>
</blockquote>
<p>从上诉的<span class="math inline">\(l_2\)</span>正则化等价于梯度衰减的过程可以看出，如果存在一个全局最优的<span class="math inline">\(\lambda\)</span>，则 <span class="math inline">\(\lambda^{&#39;}\)</span> 与学习率 <span class="math inline">\(\alpha\)</span> 紧密捆绑在一起，而Adamw做的就是将两者之间解耦。</p>
<p><img src="https://ml-explained.com/articles/adamw-explained/adamw.png" /></p>
<h1 id="radam2019">RAdam2019</h1>
<p><img src="https://ml-explained.com/_ipx/sizes_xs:320px%20md:768px%20lg:1024px,w_1536,f_png/articles/radam-explained/radam_update_rule.png" /></p>
<h1 id="参考">参考</h1>
<p>[1]http://d2l.ai/chapter_optimization/index.html <br> [2]https://machinelearningmastery.com/tour-of-optimization-algorithms/<br></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/11/15/tensor%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D/" rel="prev" title="tensor使用方法介绍">
                  <i class="fa fa-chevron-left"></i> tensor使用方法介绍
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/11/16/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" rel="next" title="激活函数">
                  激活函数 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Scofield</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
